# Plan: v7.0.0 - Create Local Dev/Test Environment

**Branch**: `7.0.0-create-local-dev-test-environment`
**Status**: Planning
**Priority**: High
**Estimated Effort**: Medium (4-6 hours)

## Problem Statement

Currently, development and testing of the Optimize My Resume system requires:
- Claude API tokens (limited to 500K per 5-hour window on free tier)
- Cloud dependency (cannot work offline)
- Token consumption for every test iteration
- Risk of running out of tokens during development

This creates friction for:
- Rapid iteration and testing
- Experimenting with different prompts/approaches
- Contributing to the project (token barrier)
- Privacy-conscious development (data leaves local machine)

## Objective

Create a complete local development environment that:
1. Uses Ollama (local AI models) instead of Claude API
2. Provides unlimited, free usage for development
3. Works offline (after initial model download)
4. Maintains feature parity with production artifact
5. Makes it easy to customize/test different AI models

## Proposed Solution

### Architecture

**Dual Environment Approach**:
- **Production**: Claude artifact (cloud, token-based) - for end users
- **Local Dev**: Vite + React + Ollama (local, free) - for developers

**Key Design Decisions**:
1. **Service Layer Pattern**: Separate Ollama API calls into `ollamaService.js`
   - Easy to swap providers
   - Testable
   - Consistent error handling

2. **Configuration-Driven Models**: Use `models.json` config file
   - No code changes to add/remove models
   - Easy for non-developers to customize
   - Supports multiple AI providers

3. **Component Reuse**: Adapt existing `Phase1ResumeAnalyzer.jsx`
   - Keep UI/UX consistent
   - Modify API layer only
   - Reduce maintenance burden

### Technical Stack

**Frontend**:
- React 19.x
- Vite 7.x (dev server & build tool)
- Tailwind CSS v4 (styling)
- Lucide React (icons)

**AI Integration**:
- Ollama (local LLM runtime)
- Models: Llama 3.1, Mistral, Gemma 2, Qwen 2.5, Phi-3

**Build Tools**:
- Vite (fast HMR, ES modules)
- PostCSS + Tailwind
- ES modules (not CommonJS)

## Implementation Plan

### Phase 1: Project Setup (1 hour)

**Tasks**:
- [x] Initialize npm project (`npm init -y`)
- [x] Install dependencies:
  - `vite @vitejs/plugin-react`
  - `react react-dom`
  - `lucide-react`
  - `tailwindcss @tailwindcss/postcss autoprefixer postcss`
- [x] Create Git branch: `7.0.0-create-local-dev-test-environment`
- [x] Configure Vite (`vite.config.js`)
- [x] Configure Tailwind v4 (`tailwind.config.js`, `postcss.config.js`)
- [x] Update `package.json`:
  - Add scripts: `dev`, `build`, `preview`
  - Change type to `"module"`
- [x] Update `.gitignore` for Vite/React artifacts

**Deliverables**:
- Working Vite dev server
- Tailwind CSS functional
- Hot module reload working

### Phase 2: Core Application Structure (1.5 hours)

**Tasks**:
- [x] Create `src/` directory structure:
  ```
  src/
  â”œâ”€â”€ components/
  â”œâ”€â”€ services/
  â”œâ”€â”€ config/
  â”œâ”€â”€ App.jsx
  â”œâ”€â”€ main.jsx
  â””â”€â”€ index.css
  ```
- [x] Create `index.html` entry point
- [x] Create `src/main.jsx` (React entry)
- [x] Create `src/index.css` (Tailwind imports)
- [x] Create `src/App.jsx` (app shell with Ollama status)

**Deliverables**:
- App renders in browser
- No console errors
- Tailwind styles apply

### Phase 3: Ollama Service Integration (1.5 hours)

**Tasks**:
- [x] Create `src/services/ollamaService.js`:
  - `checkHealth()` - Test Ollama connection
  - `listModels()` - Get installed models
  - `generate()` - Text generation
  - `chat()` - Chat completions
  - `analyzeResume()` - Resume-specific wrapper
- [x] Add comprehensive error handling:
  - Connection failures
  - Model not found
  - JSON parsing errors
  - Timeout handling
- [x] Add JSDoc comments for all methods
- [x] Set base URL: `http://localhost:11434`

**Deliverables**:
- Service successfully calls Ollama API
- All error cases handled gracefully
- Clear error messages for users

### Phase 4: Model Configuration System (30 minutes)

**Tasks**:
- [x] Create `src/config/models.json`:
  ```json
  {
    "ollama": [
      {
        "id": "llama3.1:8b",
        "name": "ðŸ¦™ Llama 3.1",
        "desc": "Recommended - Best balance",
        "recommended": true
      }
    ],
    "claude": [...]
  }
  ```
- [x] Document schema for model entries:
  - `id` (required): Ollama model name
  - `name` (required): Display name
  - `desc` (required): Short description
  - `recommended` (optional): Auto-select flag
- [x] Add validation in component

**Deliverables**:
- JSON config file loaded by app
- Easy to add/remove/reorder models
- No code changes needed for model management

### Phase 5: ResumeAnalyzer Component (2 hours)

**Tasks**:
- [x] Create `src/components/ResumeAnalyzer.jsx`
- [x] Adapt from `Phase1ResumeAnalyzer.jsx`:
  - Keep all UI/UX elements
  - Replace Claude API calls with Ollama service
  - Remove token limit displays
  - Add Ollama connection status indicator
- [x] Add features:
  - Model auto-detection (filter to installed models)
  - Connection status banner (green/red/blue states)
  - "Check Status" button to retry connection
  - Debug mode toggle
  - Enhanced error messages for Ollama-specific issues
- [x] Implement analysis flow:
  1. Check Ollama connection
  2. Verify model is installed
  3. Send resume text to Ollama
  4. Parse JSON response
  5. Display analysis results
- [x] Add error handling for:
  - Ollama not running
  - Model not found
  - JSON parsing failures
  - Empty responses
  - Network errors

**Deliverables**:
- Full resume analysis working
- Same features as production artifact
- Better error messages
- Connection status visible

### Phase 6: Documentation (1.5 hours)

**Tasks**:
- [x] Create **README-LOCAL-DEV.md**:
  - Overview and benefits
  - Prerequisites
  - Quick start
  - Project structure
  - Customizing models
  - Troubleshooting
  - Performance recommendations
  - Comparison: Local vs Production

- [x] Create **SETUP-GUIDE.md**:
  - Step-by-step installation
  - Prerequisites checklist
  - Common issues and solutions
  - Development workflow
  - Testing checklist

- [x] Create **docs/MODEL-CONFIGURATION-GUIDE.md**:
  - How to add/remove/reorder models
  - All properties explained
  - Example configurations
  - Performance considerations
  - Troubleshooting model issues
  - Real-world examples

- [x] Create **GET-STARTED.md**:
  - Quick reference
  - Fastest path to running app
  - Test checklist
  - Key features overview

- [x] Create **STATUS.md**:
  - What was created
  - Current status
  - Known issues
  - Future roadmap
  - Success criteria

- [x] Create **quick-start.sh**:
  - Automated setup script
  - Checks prerequisites
  - Installs dependencies
  - Checks Ollama status
  - Offers to pull models
  - Guides user through setup

**Deliverables**:
- 5 comprehensive guides
- Automated setup script
- All edge cases documented
- Clear troubleshooting steps

### Phase 7: Testing & Validation (1 hour)

**Tasks**:
- [ ] Test with each configured model:
  - Llama 3.1 (8B)
  - Mistral
  - Gemma 2 (9B)
  - Qwen 2.5 (7B)
  - Phi-3
- [ ] Test error scenarios:
  - Ollama not running
  - Model not installed
  - Empty resume
  - Very long resume (1000+ words)
  - Special characters in resume
- [ ] Test connection states:
  - Start app before Ollama
  - Stop Ollama while app running
  - Restart Ollama (verify "Check Status" works)
- [ ] Test model auto-detection:
  - No models installed
  - Only 1 model installed
  - All models installed
- [ ] Performance benchmarking:
  - Time each model with 400-word resume
  - Note RAM usage
  - Document in guides

**Deliverables**:
- All features tested and working
- Performance benchmarks documented
- Edge cases verified
- Known issues documented in STATUS.md

### Phase 8: Final Polish & Documentation (30 minutes)

**Tasks**:
- [ ] Review all code for:
  - Consistent style
  - Clear variable names
  - Helpful comments
  - No console.logs in production
- [ ] Review documentation for:
  - Typos and grammar
  - Clear instructions
  - All links working
  - Consistent formatting
- [ ] Create summary document:
  - `docs/v7.0.0-LOCAL-DEV-SETUP-SUMMARY.md`
  - Architecture decisions
  - Key features
  - Future enhancements
- [ ] Update main README (if needed)
- [ ] Create this plan document

**Deliverables**:
- Clean, production-ready code
- Comprehensive documentation
- Summary for handoff

## File Structure

```
optimize-my-resume/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ ResumeAnalyzer.jsx       # Main UI component (Ollama version)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ ollamaService.js         # Ollama API integration
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ models.json              # Model configuration (editable!)
â”‚   â”œâ”€â”€ App.jsx                      # App shell with status
â”‚   â”œâ”€â”€ main.jsx                     # React entry point
â”‚   â””â”€â”€ index.css                    # Tailwind imports
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ MODEL-CONFIGURATION-GUIDE.md # Model customization guide
â”‚   â””â”€â”€ v7.0.0-LOCAL-DEV-SETUP-SUMMARY.md  # Technical summary
â”œâ”€â”€ GET-STARTED.md                   # Quick start guide
â”œâ”€â”€ README-LOCAL-DEV.md              # Complete documentation
â”œâ”€â”€ SETUP-GUIDE.md                   # Step-by-step setup
â”œâ”€â”€ STATUS.md                        # Current status & roadmap
â”œâ”€â”€ quick-start.sh                   # Automated setup script
â”œâ”€â”€ index.html                       # HTML entry point
â”œâ”€â”€ package.json                     # Dependencies & scripts
â”œâ”€â”€ vite.config.js                   # Vite configuration
â”œâ”€â”€ tailwind.config.js               # Tailwind configuration
â””â”€â”€ postcss.config.js                # PostCSS configuration
```

## Expected Outcomes

### User Experience

**For Developers**:
1. Clone repo, switch to branch
2. Run `./quick-start.sh`
3. Start coding - no token limits!

**For Model Customization**:
1. Edit `src/config/models.json`
2. Add/remove models
3. Restart dev server

**For Testing**:
1. Test changes with multiple models
2. Iterate rapidly without token concerns
3. Works offline (after setup)

### Performance Targets

| Model | Resume Size | Target Time | RAM |
|-------|------------|-------------|-----|
| Llama 3.1 | 400 words | < 60s | 8GB |
| Mistral | 400 words | < 30s | 6GB |
| Phi-3 | 400 words | < 25s | 4GB |

### Quality Metrics

- **Documentation**: 5+ comprehensive guides
- **Error Handling**: All edge cases covered
- **Code Quality**: Service layer, clear separation
- **Maintainability**: Config-driven, easy to extend

## Success Criteria

This implementation is successful if:

âœ… Users can run app locally without Claude tokens
âœ… Easy to add/remove/test different models
âœ… Works completely offline (after setup)
âœ… Clear error messages guide users to fixes
âœ… Faster iteration than production environment
âœ… Feature parity with production artifact
âœ… Comprehensive documentation (no questions left unanswered)

## Risks & Mitigations

### Risk 1: JSON Parsing Failures

**Problem**: Ollama models may produce invalid JSON with complex resumes

**Mitigation**:
- Use Llama 3.1 (best JSON accuracy)
- Add retry logic
- Provide clear error messages
- Document resume length limits

### Risk 2: Performance Issues

**Problem**: Large models (70B+) may be too slow for development

**Mitigation**:
- Recommend 8-13B models
- Provide performance benchmarks
- Document RAM requirements
- Offer smaller model alternatives

### Risk 3: Setup Complexity

**Problem**: Users may struggle with Ollama installation

**Mitigation**:
- Create automated setup script
- Provide step-by-step guide
- Document all common issues
- Add connection status indicator in UI

### Risk 4: Model Availability

**Problem**: Configured models may not be installed

**Mitigation**:
- Auto-detect installed models
- Show "Missing Models" warnings
- Provide installation commands
- Filter dropdown to available models only

## Future Enhancements

### Phase 2 Features (Post v7.0.0)

1. **Bullet Optimization** - Rewrite individual bullets
2. **Job Description Matching** - Compare resume to JD
3. **Streaming Responses** - Show analysis as it generates
4. **Resume History** - Save and compare analyses
5. **Multi-Model Comparison** - Run same resume with different models

### Phase 3 Features

6. **Custom Prompts** - Let users modify analysis prompts
7. **Export Formats** - PDF, DOCX, ATS-optimized
8. **Batch Processing** - Analyze multiple resumes
9. **Automated Testing** - Unit, integration, E2E tests
10. **Performance Optimization** - Lazy loading, code splitting

## Dependencies

### Required Software

- Node.js v16+
- npm v8+
- Ollama v0.1.17+
- At least one Ollama model

### Optional Software

- Git (for version control)
- VS Code (recommended editor)

### NPM Dependencies

**Production**:
- `react` ^19.2.3
- `react-dom` ^19.2.3
- `vite` ^7.3.1
- `@vitejs/plugin-react` ^5.1.2
- `lucide-react` ^0.562.0

**Development**:
- `tailwindcss` ^4.1.18
- `@tailwindcss/postcss` ^4.1.18
- `postcss` ^8.5.6
- `autoprefixer` ^10.4.23

## Timeline

- **Phase 1-2**: 2.5 hours (Project setup + Structure)
- **Phase 3-4**: 2 hours (Service + Config)
- **Phase 5**: 2 hours (UI Component)
- **Phase 6**: 1.5 hours (Documentation)
- **Phase 7**: 1 hour (Testing)
- **Phase 8**: 0.5 hours (Polish)

**Total Estimated**: 9.5 hours (could compress to 6-8 with focus)

## Acceptance Criteria

### Functional Requirements

- [x] App runs on `npm run dev`
- [x] Connects to Ollama at localhost:11434
- [x] Detects installed models
- [x] Analyzes resumes with selected model
- [x] Displays results with same UI as production
- [x] Exports job history (XML, Markdown)
- [x] Shows helpful error messages
- [x] Works offline (after setup)

### Non-Functional Requirements

- [x] Documentation covers all use cases
- [x] Setup takes < 10 minutes for new users
- [x] Code is maintainable (service layer, config-driven)
- [x] No hardcoded values (models in config)
- [x] Clear separation of concerns
- [x] Comprehensive error handling

### Documentation Requirements

- [x] Quick start guide
- [x] Complete setup guide
- [x] Model configuration guide
- [x] Troubleshooting guide
- [x] Roadmap and status
- [x] Automated setup script

## Rollback Plan

If implementation fails or has critical issues:

1. **Keep branch**: Don't delete, mark as "experimental"
2. **Document issues**: Update STATUS.md with blockers
3. **Fallback**: Continue using production artifact
4. **Iterate**: Fix issues in new branches

No impact on main branch or production artifact.

## Post-Implementation

### Immediate Next Steps

1. User testing with multiple developers
2. Gather feedback on setup process
3. Document any missing edge cases
4. Performance tuning based on real usage

### Long-Term Maintenance

1. Update model list as new Ollama models release
2. Keep dependencies up to date
3. Add features from roadmap (Phase 2+)
4. Improve documentation based on user questions

## References

- **Ollama**: https://ollama.ai/
- **Ollama API**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Vite**: https://vitejs.dev/
- **React**: https://react.dev/
- **Tailwind CSS**: https://tailwindcss.com/

---

**Plan Created**: January 10, 2026
**Target Branch**: `7.0.0-create-local-dev-test-environment`
**Estimated Completion**: 1-2 days
**Status**: Ready for Implementation âœ…
