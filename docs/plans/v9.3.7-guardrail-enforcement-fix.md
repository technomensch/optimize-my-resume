# v9.3.7 - Guardrail Enforcement Fix (Active Structural Constraints)

**Branch:** `v9.3.7-guardrail-enforcement-fix`
**Target Release:** v9.3.7
**Type:** Patch Version (Critical Hardening)
**Priority:** CRITICAL
**Status:** Planning
**Local Issue ID:** issue-98
**GitHub Continuations:** #97 (Guardrail Hardening), #99 (ENH-008 Governance), #101 (TBD)
**Triggered By:** Production Test Failure - 2026-01-29

---

## Executive Summary

**Problem:** v9.3.6 built comprehensive guardrail documentation and enforcement mechanisms, but production testing revealed **complete enforcement failure**. The system generated content that violated guardrails despite claiming compliance.

**Root Cause:** Passive instruction-based enforcement cannot prevent LLM drift in real-world usage. Documentation alone does not force compliance.

**Solution:** Implement Four-Layer Enforcement Strategy moving from passive instructions to active structural constraints.

---

## Failure Context (v9.3.6)

### What Was Built
- 37 standardized guardrails (G1-G37) in registry format
- 3-Stage Checkpoint Pattern (Budget → Per-Bullet → Reconciliation)
- Institutional knowledge (gotchas, lessons learned, ADRs)
- Validation checklist (bo_output-validator.md)

### What Failed
Production test (2026-01-29) showed:
- Jobs left out of resume bullets
- Chronological ordering violated
- Professional summary word count unaccounted
- Action verb distribution completely ignored
- **User comment:** "i have spent 2 days hardening the guardrails and everything was ignored"

---

## Lessons Learned Integration (Jan 31 Sync)

> [!NOTE]
> **Primary Insight:** Documented guardrails (v9.3.6) failed because they were *passive*. v9.3.7 must be *active* and *structural*.

| Insight | Source | Implementation Layer |
| :--- | :--- | :--- |
| Vibe-Coding Drift | [gotchas.md](../knowledge/gotchas.md) | Layer 3 & 4 (Structural Stops) |
| Pink Elephant Problem | Research (Anthropic) | Layer 0.5 (Positive Framing) |
| Unicode Evasion | arXiv 2504.11168 | Layer 0 (Sanitization) |
| Invisible Validation | jan-29-log.md | Layer 2 (Proof of Work) |

---

## Four-Layer Enforcement Strategy

Implementation follows Gemini's analysis and extends existing 3-Stage Checkpoint pattern.

### Layer 1: Structural Prompt Logic (The Rule)

**What:** Hard mathematical limits in prompt template

**Implementation:**
```javascript
// In prompt-templates.js or bo_bullet-generation-instructions.md
GUARDRAIL_ENFORCEMENT = {
  POSITION_DISTRIBUTION: {
    rule: "Max N bullets per position (derived from total word budget)",
    validation: "Assert bullets.length === expectedCount before delivery"
  },
  CHRONOLOGICAL_ORDERING: {
    rule: "Positions must be sorted by start_date DESC",
    validation: "Assert positionList[i].startDate > positionList[i+1].startDate"
  },
  WORD_COUNT_BUDGET: {
    rule: "Total resume bullets + summary = fixed budget (e.g., 800 words)",
    validation: "Assert sum(all_content_lengths) <= BUDGET_MAX"
  },
  ACTION_VERB_DIVERSITY: {
    rule: "Max 2 bullets per verb category (Built, Led, Managed, etc)",
    validation: "Assert category_distribution.max() <= 2"
  }
}
```

**Where Enforced:**
- BEFORE generation: Pre-flight checklist validates constraints are logically possible
- DURING generation: Per-bullet gates check math is maintained
- AFTER generation: Final reconciliation validates all math before output

---

### Layer 2: "Proof of Work" Schema (The Gate)

**What:** JSON validation gates that require evidence citations

**Current Problem:** Guardrails claim compliance without showing work.

**Fix:** Extend response schema to require structural proof:

```json
{
  "position": "Senior Engineer",
  "bullets": [
    {
      "text": "Modified bullet text...",
      "charCount": 145,
      "verbCategory": "Built",
      "sourceEvidence": "Original text: 'Scales backend services across multiple regions'",
      "guardrailValidation": {
        "G1_MetricTraceability": {
          "status": "PASS",
          "metric_found": "50+ deployments",
          "validation_proof": "Metric present in source evidence"
        },
        "G5_ActionVerbQuality": {
          "status": "PASS",
          "verb_category": "Built",
          "validation_proof": "Verb is G5-approved, first use of 'Built' in position"
        },
        "G24_CharacterLimit": {
          "status": "PASS",
          "char_count": 145,
          "validation_proof": "Within 100-210 char range"
        }
      }
    }
  ],
  "reconciliation": {
    "total_bullets_generated": 12,
    "expected_bullets": 12,
    "word_count_total": 798,
    "budget_max": 800,
    "all_guardrails_passed": true,
    "validation_timestamp": "2026-01-30T15:40:00Z"
  }
}
```

**Validation Gate Logic:**
- If `guardrailValidation` missing OR `all_guardrails_passed !== true` → REJECT output
- Require explicit proof for EACH guardrail, not aggregate claim
- No "validation in thinking block" — validation must be visible in structured output

---

### Layer 3: Workflow Multi-Turn (The Pause)

**What:** Break monolithic generation into reviewable stages with user confirmation

**Current:** Single API call, user never sees intermediate state

**Fix:** Two-turn workflow:

**Turn 1: Constraint Validation**
```
INPUT:
- Resume/job history
- Target word budget
- Number of positions
- Keywords to target

OUTPUT:
- Budget allocation plan (word count per position)
- Chronological ordering strategy
- Keyword distribution map
- Constraint feasibility assessment

USER ACTION: Review and confirm plan is correct
```

**Turn 2: Constrained Generation**
```
INPUT:
- Approved budget plan from Turn 1
- Position ordering from Turn 1
- Keyword map from Turn 1
- Generate actual bullets WITHIN approved constraints

OUTPUT:
- Bullets with full guardrail validation
- Reconciliation table showing all constraints met
- Keyword coverage report

USER ACTION: Accept or request modifications
```

**Benefits:**
- User sees constraints BEFORE generation
- Impossible constraints caught early
- LLM focuses on quality within known feasible bounds
- Separation of concerns (planning vs execution)

---

### Layer 4: Modular Injection (The Knowledge)

**What:** Literal guardrail content injected as "hard code" not suggestions

**Current Problem:** Guardrails referenced as "follow these patterns" — easy to ignore

**Fix:** Inject actual guardrail definitions into prompt as pseudo-code:

```
System Instructions:
---BEGIN INJECTED GUARDRAILS---

G1_METRIC_TRACEABILITY (REQUIRED):
```javascript
// Every metric in output must trace to source evidence
if (bullet.hasMetric()) {
  assert(bullet.sourceEvidence.includes(bullet.metric));
}
```

G5_ACTION_VERB_QUALITY (REQUIRED):
```javascript
// Action verb must be in approved list and first use per category
const approvedVerbs = ['Built', 'Led', 'Managed', 'Improved', 'Collaborated'];
assert(approvedVerbs.includes(bullet.verb));
assert(positionBullets.filter(b => b.verbCategory === category).length < 3);
```

G24_CHARACTER_LIMITS (REQUIRED):
```javascript
// Every bullet must be between 100-210 characters
assert(bullet.text.length >= 100);
assert(bullet.text.length <= 210);
```

---END INJECTED GUARDRAILS---
```

**Where Injected:**
- `bo_bullet-generation-instructions.md` — Master source of truth
- PROJECT-INSTRUCTIONS.md — Gets literal copy-paste from module
- Prompt templates — Literal guardrail code included in API request

**Key Principle:** LLM sees guardrails as structural requirement code, not advisory guidance.

---

## Research-Based Hardening Additions (January 2026 Best Practices)

### Layer 0: Input Sanitization & Normalization (Pre-Processing)

**Problem:** Even commercial guardrails (Microsoft Azure Prompt Shield, Meta Prompt Guard) can be bypassed using Unicode tricks, zero-width characters, and homoglyphs.

**Implementation:**
1. Normalize all Unicode input to UTF-8
2. Strip zero-width characters (U+200B, U+200C, U+200D)
3. Detect and flag homoglyphs (visually similar but different characters)
4. Enforce input length constraints to prevent resource exhaustion
5. Log suspicious input patterns for monitoring

**Rationale:** Research shows character injection techniques achieved up to 100% evasion success against major guardrail systems (source: arXiv 2504.11168).

---

### Pink Elephant Problem: Reframe All Negative Constraints as Positive Commands

**Problem:** Negative instructions ("Don't do X", "Never output Y") can prime the model to do exactly what we're preventing.

**Current (In Plan - PROBLEMATIC):**
```
FAILURE CONDITIONS (If any are true, your output is REJECTED):
- Budget Allocation Table is missing → INVALID
- Any position (0-8) is missing → INVALID
```

**Recommended Change (POSITIVE FRAMING):**
```
SUCCESS REQUIREMENTS (All must be true for acceptance):
- Budget Allocation Table MUST appear as first output
- All 9 positions (0-8) MUST be included in chronological order
- Per-bullet validation MUST be visible for each bullet
- Final Reconciliation Table MUST appear after all bullets
```

**Why This Matters:** Anthropic's own prompt engineering best practices recommend positive framing. Instead of "Do not use markdown", say "Response should be plain text prose."

**Opus Task:** Audit all prompt templates and guardrail injection text. Replace every "DO NOT", "NEVER", "AVOID" with positive restatement.

---

### Compliance Rate Tracking & Continuous Monitoring

**Problem:** No feedback loop to detect enforcement drift over time.

**Implementation (Post-Generation):**
1. Log per-guardrail compliance (pass/fail) for each generation
2. Track aggregate compliance rate by session and guardrail
3. Alert if compliance drops below platform threshold:
   - Platform 2 (Claude Project): < 50%
   - Platform 3 (Google AI Studio): < 40%
4. Identify which guardrails fail most frequently
5. Create dashboard of compliance trends over time

**Why This Matters:** Best practices emphasize "guardrails alone don't solve the whole problem—they need observability and feedback loops to ensure performance at scale" (source: Datadog, Leanware).

**Files to Create:**
- `scripts/compliance-tracker.py` - Log and aggregate compliance metrics
- `docs/knowledge/guardrail-compliance-baseline.md` - Expected rates per platform

---

### Probabilistic Expectation Setting

**Problem:** No acknowledgment that even the best guardrails can be bypassed.

**Add to all platform documentation (Platform 2, 3, 4):**

```markdown
## Realistic Compliance Expectations

This implementation uses structural enforcement, which is significantly stronger than
documentation-only approaches (v9.3.6). However, no prompt-based guardrail achieves
100% compliance.

### Platform Compliance Rates (Based on 2025 Research)

| Platform | Method | Expected Compliance | Notes |
|----------|--------|---------------------|-------|
| Platform 1 | Multi-turn chat prompts | ~30-40% | User must enforce stages |
| Platform 2 | Claude Project + minimal prompt | ~50-60% | Better than full docs, still probabilistic |
| Platform 3 | Structured prompts + low temp | ~50-70% | Untested; needs validation |
| Platform 4 | External validation + UI gates | ~95%+ | Only truly non-bypassable option |

### Why Not 100%?

Commercial guardrails from Microsoft (Azure Prompt Shield) and Meta (Prompt Guard)
achieved 100% EVASION success rates under testing (arXiv 2504.11168). This means:
- Prompt-based enforcement is probabilistic by design
- Determined attackers can find workarounds
- External validators (code-level, UI gates) are the only non-bypassable mechanism

### Recommended Approach for High-Confidence Use

For production use cases requiring true enforcement, implement **Platform 4 (JSX GUI)**
with external validation. For chat/project-based workflows, use this system but
acknowledge the compliance ceiling and test regularly.
```

---

## Implementation Plan

### Phase 1: Update Prompt Templates (LAYER 1 + 4)

**File:** `docs/workflow-templates/bullet-generation-prompt-template.md`

**Changes:**
1. Add GUARDRAIL_ENFORCEMENT section with hard mathematical limits
2. Inject literal guardrail code (bo_bullet-generation-instructions.md content)
3. Add MUST/MUST NOT language for structural requirements
4. Remove suggestion-based language ("try to", "aim for")

**Test:** Single-turn generation with new prompt → verify guardrail violations caught

---

### Phase 2: Implement Response Validation Schema (LAYER 2)

**File:** `docs/workflow-templates/guardrail-validation-schema.md` (NEW)

**Changes:**
1. Define JSON response structure with validation gates
2. Create validator function checklist
3. Document rejection criteria (when to FAIL vs PASS)
4. Add reconciliation table template

**Implementation:**
- Extended JSON response with `guardrailValidation` field
- Per-guardrail PASS/FAIL status with proof
- `all_guardrails_passed` gate before output accepted

**Test:** Generate bullets → validate against schema → verify all guardrails have proof

---

### Phase 3: Implement Two-Turn Workflow (LAYER 3)

**File:** `docs/workflow-templates/constrained-generation-workflow.md` (NEW)

**Changes:**
1. Turn 1 prompt: Budget planning, constraint validation
2. Turn 2 prompt: Constrained generation with approved plan
3. User confirmation step between turns (if applicable)
4. Fallback logic if constraints are impossible

**Implementation:**
- First API call: Budget/planning
- Display plan to user (or validate programmatically)
- Second API call: Generation with constraints
- Return validated results

**Hardening (Active Enforcement):**
- **Skill Enforcement:** Modify `.agent/workflows/generate-bullets.md` to physically block Step 3 until Step 0 (Budget Table) is output.
- **Fail-Closed Logic:** Integrate `scripts/validate_bullets.py` into the workflow. Stop-on-error if exit code is 1.

**Test:** Multi-turn workflow → confirm constraints enforced in Turn 2

---

### Phase 4: Knowledge Injection Setup (LAYER 4)

**File:** `docs/workflow-templates/guardrail-injection-manifest.md` (NEW)

**Changes:**
1. List all guardrails (G1-G37) that need injection
2. Extract literal code from bo_bullet-generation-instructions.md
3. Create injection template for prompt builders
4. Document which guardrails are "injectable" vs "advisory"

**Implementation:**
- Manifest of guardrails to inject
- Actual injection code in prompt templates
- Verification that injected content matches source

**Test:** Prompt includes all critical guardrails as pseudo-code

---

## Files to Create/Modify

### New Files (Created)
- `docs/plans/v9.3.7-guardrail-enforcement-fix.md` (this file)
- `docs/issues/v9.3.7/` (local issue folder)
- `docs/workflow-templates/guardrail-validation-schema.md`
- `docs/workflow-templates/constrained-generation-workflow.md`
- `docs/workflow-templates/guardrail-injection-manifest.md`
- `docs/platform-implementations/v9.3.7-platform-2-claude-project.md` (NEW - Claude Project instructions with Layer 1-3)
- `docs/platform-implementations/v9.3.7-platform-3-google-ai-studio.md` (NEW - Google AI Studio instructions with Layer 1-3)

### Modified Files (Shadow Sync Tiers)

**Modular Tier (optimization-tools/)**
- `bo_bullet-generation-instructions.md` — Ensure all guardrails are injectable for Layer 4

**Gold Master Tier (PROJECT-INSTRUCTIONS.md)**
- `PROJECT-INSTRUCTIONS.md` — Reference new validation schema and workflow

**Optimized Entrypoint Tier (Project-GUI-Instructions.md)**
- `Project-GUI-Instructions.md` — Sync enforcement workflow references

**Knowledge & Documentation Tier**
- `docs/issue-tracker.md` — Add v9.3.7 entry
- `docs/knowledge/gotchas.md` — Document enforcement failure pattern
- `docs/decisions/ADR-011-structural-enforcement-over-passive.md` (NEW) — Formalize decision

### Deprecated Files (Completely Replaced by v9.3.7 Enforcement-Aware Docs)

**Files to Delete/Replace (v9.3.6 Legacy, Pre-Enforcement Failure):**
- ❌ `docs/quick-start/PROJECT-NAVIGATION-GUIDE.md` — Outdated platform comparison (no enforcement guidance)
- ❌ `docs/quick-start/QUICK-START-FIT-CHECK-BULLETS-SUMMARY.md` — Claude Chat guide without enforcement
- ❌ `docs/quick-start/CLAUDE-PROJECT-INSTRUCTIONS.md` — Replaced by v9.3.7-platform-2
- ❌ `docs/quick-start/SHOULD-I-APPLY-PROJECT-INSTRUCTIONS.md` — Subsumed into v9.3.7-platform-2
- ❌ `docs/quick-start/RESUME-ANALYZER-PROJECT-INSTRUCTIONS.md` — Subsumed into v9.3.7-platform-2

**Replacement Strategy:**
These files were created **before** the enforcement failure and contain **no enforcement guidance**. v9.3.7 will replace them with:
- `docs/platform-implementations/v9.3.7-platform-2-claude-project.md` — Enforcement-aware Claude Project setup with multi-turn gates
- `docs/platform-implementations/v9.3.7-platform-3-google-ai-studio.md` — Enforcement-aware Google AI Studio setup with structured prompts

**Rationale:** The quick-start folder assumes passive documentation-based enforcement, which v9.3.6 proved fails completely. v9.3.7 provides active structural enforcement that prevents bypass. Keeping old files creates confusion and contradicts the new enforcement model.

---

## Success Criteria

**Core Four-Layer Implementation:**
- [ ] Layer 1: Hard mathematical limits in prompt template with MUST language
- [ ] Layer 2: JSON schema with per-guardrail validation proof required
- [ ] Layer 3: Two-turn workflow implemented with constraint validation Turn 1
- [ ] Layer 4: All critical guardrails (G1-G37) injected as pseudo-code in prompt
- [ ] Validation gates reject output if guardrails not explicitly verified
- [ ] Reconciliation table shows all constraints met with proof

**Platform-Specific Outputs:**
- [ ] Platform 2 (Claude Project): Minimized 500-word system prompt created with critical guardrails only
- [ ] Platform 2: Multi-turn workflow documented with user approval gates
- [ ] Platform 2: Expected compliance notes (~60%) included with rationale
- [ ] Platform 3 (Google AI Studio): YAML structured prompt template created with input/output schema
- [ ] Platform 3: Temperature/safety settings documented for determinism
- [ ] Platform 3: Expected compliance notes (~50-70%) with testing recommendations

**Research-Based Hardening (January 2026 Best Practices):**
- [ ] Layer 0 (Input Sanitization): Unicode normalization, zero-width stripping, homoglyph detection implemented
- [ ] All negative constraints reframed as positive commands (Pink Elephant Problem fix)
- [ ] All instances of "DO NOT", "NEVER", "AVOID" replaced with positive equivalents
- [ ] Compliance tracking script created and tested

**Testing & Documentation:**
- [ ] Production test re-run: All guardrails enforced, violations caught
- [ ] Input sanitization tested against Unicode/homoglyph evasion attempts
- [ ] Positive framing validated in test outputs
- [ ] Compliance tracking baseline established for each platform
- [ ] Documentation updated to reflect structural enforcement approach
- [ ] Probabilistic expectations documented for all platforms (50-70% realistic, not 100%)
- [ ] Research findings from arXiv 2504.11168 documented in gotchas.md
- [ ] Platform comparison table updated with v9.3.7 implementation status
- [ ] Shadow sync verified across all three tiers (MODULAR, GOLD MASTER, ENTRYPOINT)
- [ ] Legacy quick-start files deleted (5 files) — replaced by enforcement-aware platform docs
- [ ] v9.3.7 platform documentation complete and ready for user onboarding

---

## Testing Strategy

### Unit Tests
- Prompt includes all guardrails as code (not just references)
- Validation schema rejects malformed responses
- Two-turn workflow preserves constraints across turns

### Integration Tests
- Full bullet generation workflow enforces all layers
- Guardrail violations caught before output
- Reconciliation table is accurate

### Production Test (Final)
- Real resume + JD (same test case as v9.3.6 failure)
- Verify no guardrail violations in output
- Confirm enforcement is visible (not hidden in thinking blocks)

---

## Risk Mitigation

**Risk:** Guardrails are too strict, generation becomes impossible

**Mitigation:** Layer 3 (multi-turn) validates feasibility in Turn 1 before committing to generation

**Risk:** LLM ignores injected code just like instructions

**Mitigation:** Make guardrails structural requirements (JSON validation gates), not request instructions. Invalid output is rejected, not accepted.

**Risk:** Performance impact from two-turn workflow

**Mitigation:** Turn 1 is fast (budget math only), Turn 2 is standard generation. Overall time: 1-2x normal.

---

## Shadow Sync Protocol Enforcement (Critical for Implementation)

**Purpose:** Prevent enforcement drift by ensuring guardrail definitions remain synchronized across all documentation tiers.

**Why This Matters:** v9.3.6 had guardrails in multiple locations with potential inconsistencies. v9.3.7 enforcement must be enforced at commit time, not hoped for after the fact.

**Three-Tier Synchronization (Required by Opus):**

1. **MODULAR Tier** - `bo_bullet-generation-instructions.md`
   - Master source of guardrail definitions (G1-G37)
   - Literal guardrail code that will be injected into prompts
   - Changes here must propagate to other tiers

2. **GOLD MASTER Tier** - `PROJECT-INSTRUCTIONS.md`
   - Central reference document for all enforcement workflows
   - Must reference the exact guardrail definitions from MODULAR tier
   - Serves as secondary validation that definitions are consistent

3. **OPTIMIZED ENTRYPOINT Tier** - `Project-GUI-Instructions.md`
   - User-facing instruction document
   - Must sync enforcement workflow references with GOLD MASTER
   - Ensures users see consistent guidance across touchpoints

**Enforcement Mechanism:**

Opus MUST use `@.agent/workflows/enforce-shadow-sync.md` when implementing any of the following:
- Changes to `bo_bullet-generation-instructions.md` (guardrail definitions)
- Updates to `PROJECT-INSTRUCTIONS.md` (central reference)
- Modifications to `Project-GUI-Instructions.md` (entrypoint sync)

**Acceptance Criteria:**
- [ ] All changes to MODULAR tier propagate to GOLD MASTER and ENTRYPOINT tiers
- [ ] Enforce-shadow-sync workflow is executed BEFORE committing
- [ ] All three tiers verified as synchronized at commit time
- [ ] No orphaned or inconsistent guardrail references remain

**Failure Prevention:**
If shadow sync is not enforced during implementation, v9.3.7 becomes another inconsistent enforcement system like v9.3.6 — guardrails documented in one place but missing or outdated in others.

---

## Related Issues

- GitHub #97 (Initial failure report)
- ADR-010 (Guardrail Hardening Pattern) - superseded by Layer approach
- v9.3.6 (Foundation work: Registry, 3-Stage Checkpoint, documentation)

---

## Task Allocation: Sonnet vs Opus

### Sonnet's Tasks (Analysis & Design)

1. **Comparison Analysis**
   - Read Gemini's Four-Layer Enforcement Strategy recommendation
   - Compare against v9.3.6 enforcement system that failed
   - Identify gaps in v9.3.6 that Gemini's layers address
   - Update plan with merged insights from both approaches

2. **Design Validation**
   - Validate Four-Layer approach is complete and non-redundant
   - Identify implementation sequence and dependencies
   - Flag any architectural risks or conflicts
   - Create detailed technical specifications for each layer

3. **Documentation Review**
   - Ensure plan aligns with start-issue-tracking.md workflow
   - Verify all steps 0-7 are properly documented
   - Check that solution-approach is complete for Opus handoff

### Opus's Tasks (Implementation)

**Pre-Implementation Hardening (Research-Based):**

0. **Input Sanitization Layer (Layer 0)**
   - Create `docs/workflow-templates/input-sanitization.md`
   - Implement Unicode normalization (UTF-8)
   - Strip zero-width characters (U+200B, U+200C, U+200D)
   - Detect homoglyphs and suspicious patterns
   - Add input length constraints
   - Document bypass prevention rationale

1. **Audit & Reframe All Constraints (Pink Elephant Problem)**
   - Scan all guardrail text for negative language ("DO NOT", "NEVER", "AVOID")
   - Reframe as positive commands ("MUST", "Always", "Required to")
   - Update Layer 1 prompt templates with positive framing
   - Update Layer 2 "Proof of Work" schema descriptions
   - Update Layer 4 guardrail code comments
   - Test reframed constraints against test cases

2. **Layer 1 & 4 Implementation** (Prompt Templates)
   - Update `bullet-generation-prompt-template.md`
   - Add hard mathematical limits (LAYER 1)
   - Inject guardrail code as pseudo-code (LAYER 4)
   - Test with single-turn generation

2. **Layer 2 Implementation** (Validation Schema)
   - Create `guardrail-validation-schema.md`
   - Implement per-guardrail PASS/FAIL gates
   - Create validator function checklist
   - Test schema with sample responses

3. **Layer 3 Implementation** (Two-Turn Workflow)
   - Create `constrained-generation-workflow.md`
   - Implement Turn 1: Budget planning
   - Implement Turn 2: Constrained generation
   - Test workflow end-to-end

4. **Platform-Specific Implementation Outputs**

   **Platform 2: Claude Project (Modified Layer 3 - Recommended Multi-Turn)**
   - Create `v9.3.7-platform-2-claude-project.md`
   - Minimized system prompt (500 words max) with critical enforcement rules only
   - Implementation guide for setting up Claude Project with reduced guardrail count (G1, G8, G12, G24, G29, G40 focus)
   - Three-tier validation: Budget Table (L1) → Per-Bullet Gates (L2) → Reconciliation (L3)
   - User approval gates between stages explicitly documented
   - **ADD: Probabilistic expectations section** (~50-60% compliance, why not 100%, commercial guardrail bypass research)
   - **ADD: Input sanitization note** (Layer 0 recommended for pre-processing)

   **Platform 3: Google AI Studio (Structured Prompts + Low Temp)**
   - Create `v9.3.7-platform-3-google-ai-studio.md`
   - YAML structured prompt template with input/output schema
   - Output schema enforcement of all 3 stages (budget, bullets, reconciliation)
   - Temperature/safety settings for maximum determinism
   - Simplified guardrail injection (Layer 4 adapted for Gemini)
   - **ADD: Probabilistic expectations section** (~50-70% compliance, untested, testing recommendations)
   - **ADD: Input sanitization note** (Layer 0 recommended for pre-processing)

5. **Compliance Tracking Implementation**
   - Create `scripts/compliance-tracker.py`
   - Log per-guardrail pass/fail status for each generation
   - Track aggregate compliance rate by session, platform, and guardrail
   - Alert thresholds: Platform 2 < 50%, Platform 3 < 40%
   - Create `docs/knowledge/guardrail-compliance-baseline.md` with expected rates
   - Implement dashboard/report for compliance trends
   - **Purpose:** Detect enforcement drift over time and identify weak guardrails

6. **Integration & Testing**
   - Run production test (v9.3.6 failure case)
   - Verify all guardrails enforced
   - Confirm violations caught before output
   - Document enforcement proof in reconciliation
   - Test input sanitization against Unicode/homoglyph evasion attempts
   - Validate positive framing (reframed constraints) in test outputs

7. **Documentation Cleanup & Migration**
   - **DELETE legacy quick-start files** (created before enforcement failure):
     * `docs/quick-start/PROJECT-NAVIGATION-GUIDE.md`
     * `docs/quick-start/QUICK-START-FIT-CHECK-BULLETS-SUMMARY.md`
     * `docs/quick-start/CLAUDE-PROJECT-INSTRUCTIONS.md`
     * `docs/quick-start/SHOULD-I-APPLY-PROJECT-INSTRUCTIONS.md`
     * `docs/quick-start/RESUME-ANALYZER-PROJECT-INSTRUCTIONS.md`
   - **Rationale:** These files assume passive enforcement, which v9.3.6 proved doesn't work. Replace with enforcement-aware platform docs.

8. **Knowledge Graph & Documentation + Shadow Sync Enforcement**
   - Update `docs/knowledge/gotchas.md` with enforcement patterns
   - Create `ADR-011-structural-enforcement-over-passive.md` (include research findings on guardrail evasion)
   - Update issue-tracker.md with completion status
   - Add research section to gotchas.md documenting arXiv 2504.11168 findings
   - Document Pink Elephant Problem and reframing approach
   - **REQUIRED:** Use `@.agent/workflows/enforce-shadow-sync.md` to synchronize:
     * MODULAR: `bo_bullet-generation-instructions.md` (guardrail content)
     * GOLD MASTER: `PROJECT-INSTRUCTIONS.md` (central reference)
     * ENTRYPOINT: `Project-GUI-Instructions.md` (user-facing sync)
   - Verify all three tiers remain consistent before committing

---

## Sonnet Analysis Results (2026-01-30)

### General Four-Layer Strategy Validation

**Finding:** The Four-Layer Enforcement Strategy directly addresses every failure mode observed in v9.3.6. The strategy is architecturally sound and complete. No additional layers required.

**Gap Analysis:**
- ✅ Layer 1 fixes: positions omitted, wrong order, budget ignored
- ✅ Layer 2 fixes: invisible validation, fake compliance claims
- ✅ Layer 3 fixes: no validation before generation, impossible constraints not caught
- ✅ Layer 4 fixes: guardrails treated as suggestions instead of requirements
- ✅ No gaps identified - all v9.3.6 failure modes covered
- ✅ Redundancy is intentional (defense-in-depth)

**Implementation Sequence Validated:** Layer 3 (workflow foundation) → Layer 1 (structural constraints) → Layer 2 (validation schema) → Layer 4 (code injection)

### Gemini Custom Keyword Strategy Comparison

**Finding:** Gemini's Four-Layer Strategy for custom keyword enforcement is architecturally **identical** to the general Four-Layer Strategy. Gemini independently applied the same pattern to a specific guardrail problem.

**Key Validation:**
- Gemini Layer 1: Keyword density limits (`Max 5 custom keywords per position`)
- Gemini Layer 2: JSON schema with `sourceEvidence` for each `customKeyword`
- Gemini Layer 3: Two-turn workflow (Keyword Map → Bullets)
- Gemini Layer 4: Inject `bo_keyword_handling.md` literally

**What This Proves:**
1. The Four-Layer architecture is generalizable (works for specific and comprehensive enforcement)
2. All four layers are necessary (Gemini needed all four for custom keywords alone)
3. The pattern is correct (independent derivation validates approach)

### Recommended Implementation Path: Hybrid Approach

**Rationale:** Gemini's recommendation suggests implementing custom keywords first as proof-of-concept, then extending to all guardrails.

**Phase 1: Custom Keywords (Quick Win)**
- Apply Four-Layer pattern to custom keyword enforcement
- Timeline: 1-2 hours
- Validates the architecture works in production
- Addresses Gemini's identified issue

**Phase 2: All Guardrails (Comprehensive)**
- Extend same architecture to all G1-G37 guardrails
- Timeline: 3-4 hours
- Addresses ALL v9.3.6 failures
- Same implementation sequence: Layer 3 → 1 → 2 → 4

**Total Timeline:** ~4-6 hours (same as original estimate)

**Benefits:**
- Early validation that architecture works (reduces Phase 2 risk)
- Immediate fix for custom keyword violations
- Full enforcement coverage after Phase 2

---

## Notes

This is the "structural constraints" pivot mentioned in the enforcement failure case study. Moving from passive documentation (v9.3.6) to active structural validation (v9.3.7).

**Key Philosophy Change:**
- v9.3.6: "Here are guardrails to follow" → Often ignored
- v9.3.7: "Your response MUST pass these gates" → Structurally enforced

The difference is validation gates that reject invalid output, not instructions that can be ignored.

**Continuation Context:**
- Builds on #97 (Guardrail Registry & Hardening)
- Extends #99 (ENH-008 Governance modularization)
- Related to #101 (TBD - enforcement framework)
- Addresses root cause failure identified in production testing

---

## Research Sources & References

### Four-Layer Enforcement Strategy
- [LLM Guardrails Best Practices 2025 | Leanware](https://www.leanware.co/insights/llm-guardrails)
- [Guide for Guardrails Implementation in 2026 | Wizsumo](https://www.wizsumo.ai/blog/how-to-implement-ai-guardrails-in-2026-the-complete-enterprise-guide)
- [The Ultimate Guide to Guardrails in GenAI | Ajay Verma | Medium](https://medium.com/@ajayverma23/the-ultimate-guide-to-guardrails-in-genai-securing-and-standardizing-llm-applications-1502c90fdc72)

### Guardrail Evasion & Limitations
- [Bypassing LLM Guardrails: Evasion Attacks Analysis | arXiv 2504.11168](https://arxiv.org/html/2504.11168v1) — **Critical:** Commercial guardrails (Microsoft Azure Prompt Shield, Meta Prompt Guard) achieved 100% evasion success. This proves prompt-based enforcement is probabilistic.
- [LLM Salting: New Technique to Prevent Jailbreaks | Sophos](https://www.sophos.com/en-us/blog/locking-it-down-a-new-technique-to-prevent-llm-jailbreaks)
- [LLM Security in 2025: Prompt Injection & Jailbreaks | Elysiate](https://www.elysiate.com/blog/llm-security-prompt-injection-jailbreaking-prevention)

### Pink Elephant Problem & Positive Framing
- [The Pink Elephant Problem: Why "Don't Do That" Fails with LLMs | 16x Engineer](https://eval.16x.engineer/blog/the-pink-elephant-negative-instructions-llms-effectiveness-analysis) — **Critical:** Negative instructions can prime the model to do what we're preventing. Anthropic recommends positive framing.

### Input Sanitization & Unicode Attacks
- [LLM Prompt Injection Prevention | OWASP](https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html)
- [LLM Security Checklist | Lookout](https://www.lookout.com/blog/jailbreaking)
- [LLM Guardrails: Safeguard LLM Systems | Confident AI](https://www.confident-ai.com/blog/llm-guardrails-the-ultimate-guide-to-safeguard-llm-systems)

### Monitoring & Observability
- [LLM Guardrails Best Practices | Datadog](https://www.datadoghq.com/blog/llm-guardrails-best-practices/) — Emphasizes that guardrails need monitoring and feedback loops at scale.
- [LLM Guardrail Testing in 2025 | Avido AI](https://avidoai.com/blog/llm-guardrail-testing) — Financial services perspective on compliance testing.
