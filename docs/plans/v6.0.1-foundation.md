# v6.0.1 Foundation - Schema & Core Infrastructure

**Version:** 6.0.1 (Part 1 of 4)
**Branch:** `v6.0.1-foundation`
**Token Budget:** 53,000 tokens (within 200K limit)
**Status:** Ready for Implementation

---

## Overview

This is the first phase of the v6.0 complete workflow system. It establishes the foundational schemas and parsing logic that all other phases will build upon.

**Goal:** Create the core data structures and parsing modules without modifying existing modes.

**Strategy:** Build in isolation - all new files, no modifications to existing code. This allows safe testing and rollback.

---

## Scope

### ✅ Included in v6.0.1

1. **Job History Schema v2.0** - Complete 12-section schema with hard/soft skills separation
2. **Skills Categorization Module** - Hard vs soft skill classification with confidence scoring
3. **17-Point JD Parser** - Full extraction schema from legacy system

### ❌ Not Included (Deferred to Later Phases)

- Mode modifications (v6.0.2)
- Entry point router (v6.0.3)
- Professional summary generation (v6.0.4)
- Integration with existing workflows (v6.0.2+)

---

## Files to Create (3 New Files)

### File 1: `/shared/schemas/job-history-v2.0-schema.json`

**Purpose:** JSON schema definition for job history v2.0 with validation rules

**Size:** ~250 lines

**Content Structure:**
```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Job History Schema v2.0",
  "version": "2.0.0",
  "type": "object",
  "required": ["schema_version", "positions"],
  "properties": {
    "schema_version": {
      "type": "string",
      "const": "2.0.0"
    },
    "positions": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["job_title", "company", "dates"],
        "properties": {
          "job_title": {"type": "string"},
          "company": {"type": "string"},
          "dates": {"type": "string"},
          "professional_summary": {"type": "string"},
          "core_responsibilities": {
            "type": "array",
            "items": {"type": "string"}
          },
          "key_achievements": {
            "type": "array",
            "items": {"type": "string"}
          },
          "hard_skills_demonstrated": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Technical/measurable skills (Python, SQL, AWS, etc.)"
          },
          "soft_skills_demonstrated": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Interpersonal/behavioral skills (Leadership, Communication, etc.)"
          },
          "education": {
            "type": "string",
            "description": "Degree, institution, year"
          },
          "certifications": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Professional certifications"
          },
          "tools_technologies": {
            "type": "array",
            "items": {"type": "string"}
          },
          "impact_metrics": {
            "type": "array",
            "items": {"type": "string"}
          },
          "industry_domain": {"type": "string"},
          "team_scope": {"type": "string"}
        }
      }
    }
  }
}
```

**Key Features:**
- Semantic versioning (2.0.0)
- Hard/soft skills separation
- Education and certifications fields
- Validation rules for required fields

---

### File 2: `/shared/modules/skills-categorizer.md`

**Purpose:** Decision tree logic for classifying skills as hard or soft

**Size:** ~150 lines

**Content Structure:**
```xml
<skills_categorizer>
  <purpose>
    Classify each skill as HARD (technical/measurable) or SOFT (interpersonal/behavioral)
    with confidence scoring to flag ambiguous cases.
  </purpose>

  <categorization_decision_tree>
    <step_1_exact_match>
      Check against known skills database:

      HARD_SKILLS_DATABASE = [
        "Python", "Java", "JavaScript", "SQL", "C++", "Ruby", "PHP",
        "AWS", "Azure", "GCP", "Docker", "Kubernetes", "Terraform",
        "JIRA", "Confluence", "Tableau", "Salesforce", "SAP", "Excel",
        "HIPAA", "GDPR", "SOX", "ISO 27001", "Agile", "Scrum", "Six Sigma",
        "PMP", "CPA", "CFA", "AWS Solutions Architect", ...
      ]

      SOFT_SKILLS_DATABASE = [
        "Leadership", "Communication", "Teamwork", "Problem-solving",
        "Time management", "Adaptability", "Critical thinking",
        "Stakeholder management", "Conflict resolution", "Mentoring",
        "Delegation", "Active listening", "Empathy", ...
      ]

      IF skill in HARD_SKILLS_DATABASE:
        RETURN {category: "hard", confidence: "high"}
      ELSE IF skill in SOFT_SKILLS_DATABASE:
        RETURN {category: "soft", confidence: "high"}
      ELSE:
        CONTINUE to step_2_context_analysis
    </step_1_exact_match>

    <step_2_context_analysis>
      Check surrounding words in JD text:

      HARD_SKILL_CONTEXT = [
        "certification", "proficiency", "years experience",
        "proficient in", "expertise in", "knowledge of"
      ]

      SOFT_SKILL_CONTEXT = [
        "ability to", "demonstrated", "proven track record",
        "strong", "excellent", "effective"
      ]

      IF context matches HARD_SKILL_CONTEXT:
        RETURN {category: "hard", confidence: "medium"}
      ELSE IF context matches SOFT_SKILL_CONTEXT:
        RETURN {category: "soft", confidence: "medium"}
      ELSE:
        CONTINUE to step_3_linguistic_analysis
    </step_2_context_analysis>

    <step_3_linguistic_analysis>
      Analyze part of speech:

      IF skill is a noun (Python, SQL, Tableau):
        RETURN {category: "hard", confidence: "medium"}
      ELSE IF skill is an adjective/adverb (collaborative, efficiently):
        RETURN {category: "soft", confidence: "medium"}
      ELSE IF skill is a verb (communicate, lead, manage):
        Extract implied skill → categorize as soft
        RETURN {category: "soft", confidence: "low"}
    </step_3_linguistic_analysis>

    <step_4_default_fallback>
      When in doubt, default to HARD skill (safer for blocking gate logic)

      RETURN {category: "hard", confidence: "low", flag: "review_needed"}
    </step_4_default_fallback>
  </categorization_decision_tree>

  <validation_rules>
    <!-- Check for skills appearing in both hard and soft arrays -->
    <duplicate_check>
      IF skill appears in BOTH hard_skills and soft_skills:
        WARN "Ambiguous skill detected: {skill}"
        KEEP in hard_skills (safer default)
        REMOVE from soft_skills
    </duplicate_check>

    <!-- Flag low-confidence categorizations for human review -->
    <confidence_flag>
      IF confidence == "low":
        ADD to review_queue: {skill, category, reasoning}
    </confidence_flag>
  </validation_rules>

  <output_format>
    {
      "skill": "Data Analysis",
      "category": "hard",
      "confidence": "medium",
      "reasoning": "Paired with 'proficiency' and '3+ years experience'"
    }
  </output_format>
</skills_categorizer>
```

**Key Features (from Opus review):**
- Three-tier decision tree (exact match → context → linguistic)
- Confidence scoring (high/medium/low)
- Ambiguous skill detection
- Safe default (hard skills when uncertain)

---

### File 3: `/shared/modules/jd-processing.md`

**Purpose:** 17-point JD parsing with keyword extraction and inference

**Size:** ~250 lines

**Content Structure:**
```xml
<jd_parser_17_point>
  <purpose>
    Extract structured data from job description text using the legacy 17-point schema.
    Handle both structured and conversational JD formats.
  </purpose>

  <schema_17_point>
    <!-- Company & Role Info (4 fields) -->
    <company type="string">Employer name</company>
    <job_title type="string">Role title</job_title>
    <location type="string">Physical location or "Remote"</location>
    <work_lifestyle type="enum">Remote | On-Site | Hybrid</work_lifestyle>

    <!-- Work Conditions (4 fields) -->
    <remote_restrictions type="string">State restrictions or "fake remote" indicators</remote_restrictions>
    <employee_type type="enum">Full-time | Part-time | Contract</employee_type>
    <travel_required type="string">Travel percentage or frequency</travel_required>
    <clearance type="string">Security clearance requirements</clearance>

    <!-- Compensation & Requirements (4 fields) -->
    <salary_range type="string">Compensation range</salary_range>
    <required_experience type="string">Years and type of experience</required_experience>
    <required_education type="string">Degree requirements</required_education>
    <job_responsibilities type="array">Core duties</job_responsibilities>

    <!-- Hard Skills (2 fields) -->
    <skills_needed type="array">REQUIRED hard skills</skills_needed>
    <skills_wanted type="array">PREFERRED hard skills</skills_wanted>

    <!-- Soft Skills (2 fields) -->
    <soft_skills_needed type="array">REQUIRED soft skills</soft_skills_needed>
    <soft_skills_wanted type="array">PREFERRED soft skills</soft_skills_wanted>

    <!-- Qualifications & Certifications (4 fields) -->
    <qualifications_needed type="array">Required qualifications</qualifications_needed>
    <qualifications_wanted type="array">Preferred qualifications</qualifications_wanted>
    <certifications_needed type="array">Required certifications</certifications_needed>
    <certifications_wanted type="array">Preferred certifications</certifications_wanted>
  </schema_17_point>

  <extraction_strategy>
    <structured_jd_parsing>
      <!-- For JDs with clear sections (Requirements, Qualifications, etc.) -->

      STEP 1: Identify section headers
      - "Requirements", "Qualifications", "Responsibilities", "Nice to Have"

      STEP 2: Extract from structured sections
      - skills_needed ← "Requirements" section
      - skills_wanted ← "Nice to Have" or "Preferred" section
      - job_responsibilities ← "Responsibilities" section

      STEP 3: Categorize skills using skills-categorizer.md
      - Run each skill through categorization logic
      - Separate into hard vs soft skill arrays

      STEP 4: Fill metadata fields
      - company ← Extract from header or signature
      - location ← Look for "Location:", "Based in", address patterns
      - work_lifestyle ← "Remote", "Hybrid", "On-site" keywords
    </structured_jd_parsing>

    <conversational_jd_parsing>
      <!-- For JDs without clear structure (narrative/conversational format) -->

      STEP 1: Keyword-based extraction
      - Scan full text for skill keywords (Python, SQL, Leadership, etc.)
      - Add to skills_needed if mentioned in requirements context

      STEP 2: Pattern matching
      - "X years of experience in [SKILL]" → skills_needed
      - "Bonus if you know [SKILL]" → skills_wanted
      - "Proficient in [TOOL]" → skills_needed

      STEP 3: Contextual inference
      - "We're looking for a Python developer" → skills_needed: ["Python"]
      - "Love working with AWS?" → skills_wanted: ["AWS"]

      STEP 4: Confidence flagging
      - Mark extraction as "low confidence"
      - Warn user: "JD format is non-standard. Results may be incomplete."
    </conversational_jd_parsing>

    <missing_fields_handling>
      <!-- Per Opus Decision 6: Null + Infer -->

      IF field not found:
        - salary_range → null (continue silently)
        - clearance → null (continue silently)
        - location → null (continue silently)
        - skills_needed → Infer from job_responsibilities text
        - certifications → null (continue silently)

      NO warnings about incomplete JDs - work with what's available
    </missing_fields_handling>
  </extraction_strategy>

  <hard_soft_skill_rules>
    <!-- Critical: Correct classification affects blocking gates -->

    <hard_skills>
      DEFINITION: Technical, measurable, teachable skills

      EXAMPLES:
      - Programming languages (Python, Java, JavaScript, SQL, C++, Ruby)
      - Cloud platforms (AWS, Azure, GCP)
      - Tools/frameworks (Docker, Kubernetes, React, Django, Tableau)
      - Domain knowledge (HIPAA, GDPR, Agile, Six Sigma, SEO)
      - Technical certifications (PMP, AWS Solutions Architect, CPA)

      WHEN IN DOUBT: If it can be tested or measured, it's HARD
    </hard_skills>

    <soft_skills>
      DEFINITION: Interpersonal, behavioral, personality traits

      EXAMPLES:
      - Communication (written, verbal, presentation, stakeholder management)
      - Leadership (team management, mentoring, coaching, delegation)
      - Collaboration (teamwork, cross-functional, conflict resolution)
      - Work style (time management, adaptability, problem-solving, critical thinking)

      WHEN IN DOUBT: If it describes how someone works with others, it's SOFT
    </soft_skills>

    <classification_instruction>
      You MUST categorize each skill correctly. Misclassification affects blocking logic.

      RULE: When uncertain → default to HARD skill (safer for blocking gate calculations)
    </classification_instruction>
  </hard_soft_skill_rules>

  <output_format>
    {
      "jd_parsed": {
        "company": "TechCorp Inc.",
        "job_title": "Senior Product Manager",
        "location": "Remote",
        "work_lifestyle": "Remote",
        "skills_needed": ["Python", "SQL", "AWS", "Agile"],
        "skills_wanted": ["Kubernetes", "Terraform"],
        "soft_skills_needed": ["Leadership", "Communication"],
        "soft_skills_wanted": ["Stakeholder management"],
        "remote_restrictions": "US only",
        "salary_range": "$120K-$160K",
        "required_experience": "5+ years in product management",
        "required_education": "Bachelor's degree",
        "certifications_wanted": ["PMP", "Certified Scrum Master"]
      },
      "extraction_confidence": "high",
      "extraction_method": "structured"
    }
  </output_format>
</jd_parser_17_point>
```

**Key Features (from Opus review):**
- Dual extraction strategy (structured vs conversational)
- Inference for missing skills sections
- Null handling for optional fields
- Hard/soft skill separation with classification rules

---

## Implementation Steps

### Step 1: Create Schema File
```bash
# Create schema directory if it doesn't exist
mkdir -p shared/schemas

# Create job-history-v2.0-schema.json
# Write complete JSON schema with validation rules
```

**Validation:**
- Schema is valid JSON
- All 12 sections defined
- Required fields marked
- Version is "2.0.0"

---

### Step 2: Create Skills Categorizer Module
```bash
# Create modules directory if it doesn't exist
mkdir -p shared/modules

# Create skills-categorizer.md
# Write decision tree logic with confidence scoring
```

**Validation:**
- Test with 20 sample skills (10 hard, 10 soft)
- Verify confidence scoring works
- Check ambiguous skill detection

---

### Step 3: Create JD Parser Module
```bash
# Create jd-processing.md in shared/modules/
# Write 17-point extraction logic
```

**Validation:**
- Test with structured JD (all 17 fields present)
- Test with conversational JD (narrative format)
- Test with minimal JD (only job title and requirements)
- Verify hard/soft skill separation

---

## Testing Checklist

### Schema Validation Tests

- [ ] Schema validates valid v2.0 job history
- [ ] Schema rejects invalid data (missing required fields)
- [ ] All 12 sections defined with correct types
- [ ] Version field is "2.0.0"

### Skills Categorizer Tests

- [ ] Exact match: "Python" → hard (high confidence)
- [ ] Exact match: "Leadership" → soft (high confidence)
- [ ] Context: "Proficiency in Data Analysis" → hard (medium confidence)
- [ ] Context: "Strong communication skills" → soft (medium confidence)
- [ ] Ambiguous: "Project Management" → hard (default, low confidence)
- [ ] Duplicate detection: Skill in both hard/soft arrays → keeps hard only
- [ ] Confidence flagging: Low confidence skills added to review queue

### JD Parser Tests

**Structured JD:**
- [ ] Extracts all 17 fields from well-formatted JD
- [ ] Separates skills into hard vs soft correctly
- [ ] Distinguishes "required" vs "preferred" (needed vs wanted)
- [ ] Handles missing optional fields (salary, clearance) with null

**Conversational JD:**
- [ ] Extracts skills from narrative text
- [ ] Infers skills from responsibilities when skills section missing
- [ ] Flags as "low confidence" extraction
- [ ] Still produces usable 17-point output

**Edge Cases:**
- [ ] JD with no skills section → infers from responsibilities
- [ ] JD with mixed hard/soft in same list → categorizes correctly
- [ ] JD with ambiguous skills → defaults to hard, flags for review

---

## Dependencies

**Requires:**
- None (this phase is self-contained)

**Required by:**
- v6.0.2 (Mode 1 will use job-history schema)
- v6.0.2 (Mode 3 will use JD parser)
- v6.0.4 (Summary generator will use skills categorizer)

---

## Success Criteria

v6.0.1 is successful if:

1. ✅ Schema file validates correctly with JSON Schema validator
2. ✅ Skills categorizer achieves >95% accuracy on test set of 100 skills
3. ✅ JD parser extracts all 17 fields from 100% of structured JDs
4. ✅ JD parser extracts ≥12 fields from >80% of conversational JDs
5. ✅ Hard/soft skill separation is correct in >90% of cases
6. ✅ All tests pass without errors

---

## Issues & Mitigations

### Issue 1: Hard/Soft Skill Ambiguity (from Opus review)

**Problem:** Skills like "Agile" could be hard (methodology certification) or soft (agile mindset).

**Mitigation:**
- Context analysis in decision tree
- Confidence scoring to flag ambiguous cases
- Default to hard skill when uncertain (safer for blocking gates)

### Issue 2: Conversational JD Extraction Incomplete

**Problem:** Narrative JDs might not have all 17 fields extractable.

**Mitigation:**
- Inference from responsibilities text
- Keyword-based fallback extraction
- Clear confidence flagging ("low confidence" warning)
- Allow nulls for optional fields (salary, clearance, location)

---

## Next Steps

**After v6.0.1 Completes:**

1. **Git Workflow:**
   ```bash
   git checkout -b v6.0.1-foundation
   git add shared/schemas/ shared/modules/
   git commit -m "feat(v6.0.1): add job history schema v2.0, skills categorizer, and JD parser"
   git push origin v6.0.1-foundation
   ```

2. **Proceed to v6.0.2:**
   - Integrate schema into Mode 1 (job history generation)
   - Integrate JD parser into Mode 3 (gap analysis)
   - See `/docs/plans/v6.0.2-core-integration.md`

---

**Plan Created:** 2025-12-28
**Token Budget:** 53,000 tokens (estimated)
**Dependencies:** None
**Next Phase:** v6.0.2 Core Integration
